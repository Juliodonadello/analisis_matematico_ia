{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6833e4b-0d0f-49aa-8b3e-0aef4930044d",
   "metadata": {},
   "source": [
    "# Implementación base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f9f017-d2ac-4e30-93b3-4ac4e34c5f1f",
   "metadata": {},
   "source": [
    "1. Entrenar un modelo QDA sobre el dataset *iris* utilizando las distribuciones *a priori* a continuación ¿Se observan diferencias?¿Por qué cree? _Pista: comparar con las distribuciones del dataset completo, **sin splitear**_.\n",
    "    1. Uniforme (cada clase tiene probabilidad 1/3)\n",
    "    2. Una clase con probabilidad 0.9, las demás 0.05 (probar las 3 combinaciones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "245b4960-2968-4339-8827-9b9d27ee2216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import det, inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0f9d8e70-89b0-4153-89ab-c6b604baa24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassEncoder:\n",
    "  def fit(self, y):\n",
    "    self.names = np.unique(y)\n",
    "    self.name_to_class = {name:idx for idx, name in enumerate(self.names)}\n",
    "    self.fmt = y.dtype\n",
    "    # Q1: por que no hace falta definir un class_to_name para el mapeo inverso?\n",
    "\n",
    "  def _map_reshape(self, f, arr):\n",
    "    return np.array([f(elem) for elem in arr.flatten()]).reshape(arr.shape)\n",
    "    # Q2: por que hace falta un reshape?\n",
    "    # A2: para que el return de _map_reshape devuelva un array con las mismas dimensiones\n",
    "    #     que el el input (arr). Esto le da consistencia e interoperabilidad al metodo.\n",
    "\n",
    "  def transform(self, y):\n",
    "    return self._map_reshape(lambda name: self.name_to_class[name], y)\n",
    "\n",
    "  def fit_transform(self, y):\n",
    "    self.fit(y)\n",
    "    return self.transform(y)\n",
    "\n",
    "  def detransform(self, y_hat):\n",
    "    return self._map_reshape(lambda idx: self.names[idx], y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9d444de9-c88a-4c09-9a6b-5da315e3efad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseBayesianClassifier:\n",
    "  def __init__(self):\n",
    "    self.encoder = ClassEncoder()\n",
    "\n",
    "  def _estimate_a_priori(self, y):\n",
    "    a_priori = np.bincount(y.flatten().astype(int)) / y.size\n",
    "    # Q3: para que sirve bincount?\n",
    "    return np.log(a_priori)\n",
    "\n",
    "  def _fit_params(self, X, y):\n",
    "    # estimate all needed parameters for given model\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  def _predict_log_conditional(self, x, class_idx):\n",
    "    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n",
    "    # this should depend on the model used\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  def fit(self, X, y, a_priori=None):\n",
    "    # first encode the classes\n",
    "    y = self.encoder.fit_transform(y)\n",
    "\n",
    "    # if it's needed, estimate a priori probabilities\n",
    "    self.log_a_priori = self._estimate_a_priori(y) if a_priori is None else np.log(a_priori)\n",
    "\n",
    "    # check that a_priori has the correct number of classes\n",
    "    assert len(self.log_a_priori) == len(self.encoder.names), \"A priori probabilities do not match number of classes\"\n",
    "\n",
    "    # now that everything else is in place, estimate all needed parameters for given model\n",
    "    self._fit_params(X, y)\n",
    "    # Q4: por que el _fit_params va al final? no se puede mover a, por ejemplo, antes de la priori?\n",
    "    return self.encoder.name_to_class\n",
    "\n",
    "  def predict(self, X):\n",
    "    # this is actually an individual prediction encased in a for-loop\n",
    "    m_obs = X.shape[1]\n",
    "    y_hat = np.empty(m_obs, dtype=self.encoder.fmt)\n",
    "\n",
    "    for i in range(m_obs):\n",
    "      encoded_y_hat_i = self._predict_one(X[:,i].reshape(-1,1))\n",
    "      y_hat[i] = self.encoder.names[encoded_y_hat_i]\n",
    "\n",
    "    # return prediction as a row vector (matching y)\n",
    "    return y_hat.reshape(1,-1)\n",
    "\n",
    "  def _predict_one(self, x):\n",
    "    # calculate all log posteriori probabilities (actually, +C)\n",
    "    log_posteriori = [ log_a_priori_i + self._predict_log_conditional(x, idx) for idx, log_a_priori_i\n",
    "                  in enumerate(self.log_a_priori) ]\n",
    "\n",
    "    # return the class that has maximum a posteriori probability\n",
    "    return np.argmax(log_posteriori)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ca0809d2-f481-4fc7-b1fe-37140866c8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QDA(BaseBayesianClassifier):\n",
    "\n",
    "  def _fit_params(self, X, y):\n",
    "    # estimate each covariance matrix\n",
    "    self.inv_covs = [inv(np.cov(X[:,y.flatten()==idx], bias=True))\n",
    "                      for idx in range(len(self.log_a_priori))]\n",
    "    # Q5: por que hace falta el flatten y no se puede directamente X[:,y==idx]?\n",
    "    # Q6: por que se usa bias=True en vez del default bias=False?\n",
    "    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True)\n",
    "                  for idx in range(len(self.log_a_priori))]\n",
    "    # Q7: que hace axis=1? por que no axis=0?\n",
    "\n",
    "  def _predict_log_conditional(self, x, class_idx):\n",
    "    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n",
    "    # this should depend on the model used\n",
    "    inv_cov = self.inv_covs[class_idx]\n",
    "    unbiased_x =  x - self.means[class_idx]\n",
    "    return 0.5*np.log(det(inv_cov)) -0.5 * unbiased_x.T @ inv_cov @ unbiased_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d6ac65-c370-4f78-b8f3-ef3a0ca97810",
   "metadata": {},
   "source": [
    "### 1. Seteo de datos - Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e2119fb1-be8e-4fc5-aab0-78806a7b2604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hiperparámetros\n",
    "rng_seed = 6543"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "743d68c1-a6f1-4abe-9759-07856a388904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (150, 4), Y:(150, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "def get_iris_dataset():\n",
    "  data = load_iris()\n",
    "  X_full = data.data\n",
    "  y_full = np.array([data.target_names[y] for y in data.target.reshape(-1,1)])\n",
    "  return X_full, y_full\n",
    "\n",
    "X_full_iris, y_full_iris = get_iris_dataset()\n",
    "\n",
    "print(f\"X: {X_full_iris.shape}, Y:{y_full_iris.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "28c35957-4441-4949-940a-1748de602273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (4, 150), Y:(1, 150)\n"
     ]
    }
   ],
   "source": [
    "def transpose(X, y):\n",
    "    # transpose so observations are column vectors\n",
    "    return X.T, y.T\n",
    "    \n",
    "def accuracy(y_true, y_pred):\n",
    "  return (y_true == y_pred).mean()\n",
    "\n",
    "X_full_iris, y_full_iris = transpose(X_full_iris, y_full_iris)\n",
    "print(f\"X: {X_full_iris.shape}, Y:{y_full_iris.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a00e9a-8345-4b13-b58d-92f15e60ffdc",
   "metadata": {},
   "source": [
    "### 1. A) Uniforme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b44909c-9301-45a3-b542-8a85d22e5a11",
   "metadata": {},
   "source": [
    "___Como indica la consigna, vamos a trabajar sin splitear.___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e2e06436-459e-4395-add1-0b6bf7a23800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'setosa': 0, 'versicolor': 1, 'virginica': 2}\n",
      "Train (apparent) error for iris with Uniform likelihood is 0.0200\n"
     ]
    }
   ],
   "source": [
    "#Entrenamos un QDA y medimos su accuracy\n",
    "\n",
    "qda = QDA()\n",
    "names_class = qda.fit(X_full_iris, y_full_iris, a_priori=np.array([1/3, 1/3, 1/3]))\n",
    "print(names_class)\n",
    "train_acc = accuracy(y_full_iris, qda.predict(X_full_iris))\n",
    "print(f\"Train (apparent) error for iris with Uniform prior is {1-train_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ac8bb0-d3bd-4c5d-90ae-cad8955eda8e",
   "metadata": {},
   "source": [
    "### 1. B1) 0.9 0.05 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1dbc5b8d-a6d8-4803-983f-564b283affe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (apparent) error for iris with B1 likelihood is 0.0200\n"
     ]
    }
   ],
   "source": [
    "#Entrenamos un QDA y medimos su accuracy\n",
    "\n",
    "qda = QDA()\n",
    "qda.fit(X_full_iris, y_full_iris, a_priori=np.array([0.9, 0.05, 0.05]))\n",
    "train_acc = accuracy(y_full_iris, qda.predict(X_full_iris))\n",
    "print(f\"Train (apparent) error for iris with B1 prior is {1-train_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caea083-3578-4546-a4bc-7286c9792b61",
   "metadata": {},
   "source": [
    "### 1. B2) 0.05 0.9 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "acc3bbf0-82d6-407c-9986-50d1b7d16aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (apparent) error for iris with B2 likelihood is 0.0333\n"
     ]
    }
   ],
   "source": [
    "#Entrenamos un QDA y medimos su accuracy\n",
    "\n",
    "qda = QDA()\n",
    "qda.fit(X_full_iris, y_full_iris, a_priori=np.array([0.05, 0.9, 0.05]))\n",
    "\n",
    "train_acc = accuracy(y_full_iris, qda.predict(X_full_iris))\n",
    "print(f\"Train (apparent) error for iris with B2 prior is {1-train_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51beeb7-9dd4-41e6-9043-d44b883bfc9c",
   "metadata": {},
   "source": [
    "### 1. B3) 0.05 0.05 0.9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1cec0a17-4d51-486e-850b-fbe0ab9c8e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (apparent) error for iris with B3 likelihood is 0.0400\n"
     ]
    }
   ],
   "source": [
    "#Entrenamos un QDA y medimos su accuracy\n",
    "\n",
    "qda = QDA()\n",
    "qda.fit(X_full_iris, y_full_iris, a_priori=np.array([0.05, 0.05, 0.9]))\n",
    "\n",
    "train_acc = accuracy(y_full_iris, qda.predict(X_full_iris))\n",
    "print(f\"Train (apparent) error for iris with B3 prior is {1-train_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ebdd6637-c5d0-40e3-96c4-4d0cc785b492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('setosa', 50), ('versicolor', 50), ('virginica', 50))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vemos la distribucion de las clases de iris\n",
    "unique_values, counts = np.unique(y_full, return_counts=True)\n",
    "tuple(zip(unique_values, counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52604dad-5a10-47cf-83d0-2e3e78e61c3d",
   "metadata": {},
   "source": [
    "(RTA)  \n",
    "Comparando las corridas, se observan variaciones en los resultados del error aparente de training en los casos B2 y B3. En el caso B1, el error es identico al que se da en A (cuando asumimos una distribución uniforme). \n",
    "El caso A responde a la máxima verosimilitud, ya que el dataset posee esa distribución (50/150 para cada clase). Sin embargo, coincide el error aparente con el caso B1. Lo cual es notorio ya que este último caso asume una distribución a priori del 90% para la clase setosa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c186cca-120d-43f4-ac3e-83ec4447b168",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18816a6e-e143-42bc-b5de-6dbd8bef8ffb",
   "metadata": {},
   "source": [
    "2. Repetir el punto anterior para el dataset penguin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498cdcde-c658-43c2-926a-14ddd97ea94e",
   "metadata": {},
   "source": [
    "___Procediendo con el dataset penguins___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "35f4526d-fb35-4e0a-bdda-2b1ac6b17dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "def get_penguins():\n",
    "    # get data\n",
    "    df, tgt = fetch_openml(name=\"penguins\", return_X_y=True, as_frame=True, parser='auto')\n",
    "\n",
    "    # drop non-numeric columns\n",
    "    df.drop(columns=[\"island\",\"sex\"], inplace=True)\n",
    "\n",
    "    # drop rows with missing values\n",
    "    mask = df.isna().sum(axis=1) == 0\n",
    "    df = df[mask]\n",
    "    tgt = tgt[mask]\n",
    "\n",
    "    return df.values, tgt.to_numpy().reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3c49feb3-deea-4494-a3eb-dc7d7e3ceba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (342, 4), Y:(342, 1)\n"
     ]
    }
   ],
   "source": [
    "X_full_peng, y_full_peng = get_penguins()\n",
    "print(f\"X: {X_full_peng.shape}, Y:{y_full_peng.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "54c95869-c090-453c-8773-1f3cb15e4a71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('Adelie', 151, '0.4415 %'),\n",
       " ('Chinstrap', 68, '0.1988 %'),\n",
       " ('Gentoo', 123, '0.3596 %'))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vemos la distribución de las clases de penguins\n",
    "unique_values, counts = np.unique(y_full_peng, return_counts=True)\n",
    "tuple(zip(unique_values, counts, (str(round(count/y_full_peng.shape[1],4))+' %' for count in counts)))\n",
    "#como son 3 clases, podemos reutilizar las distribuciones del caso anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e7fb85-d8a9-46d7-a6d4-3dd0234ee5e8",
   "metadata": {},
   "source": [
    "En este caso la distribución no es uniforme, por lo que debería aumentar el error al elegir la distribución a priori \"A\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e6372131-979b-4de5-aa43-f65f4f0bd1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (4, 342), Y:(1, 342)\n"
     ]
    }
   ],
   "source": [
    "# Volvemos a transponer para tenes las observaciones en vectores fila (a lo largo de las columnas)\n",
    "X_full_peng, y_full_peng = transpose(X_full_peng, y_full_peng)\n",
    "print(f\"X: {X_full_peng.shape}, Y:{y_full_peng.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23570ac3-5c94-4196-b9e2-05a0ce01ac44",
   "metadata": {},
   "source": [
    "### 2. A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f3fc6c71-64af-4f31-8e65-14ed7dd02fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Adelie': 0, 'Chinstrap': 1, 'Gentoo': 2}\n",
      "Train (apparent) error is 0.0088\n"
     ]
    }
   ],
   "source": [
    "#Entrenamos un QDA y medimos su accuracy\n",
    "qda = QDA()\n",
    "names_class = qda.fit(X_full_peng, y_full_peng, a_priori=np.array([1/3, 1/3, 1/3]))\n",
    "print(names_class)\n",
    "train_acc = accuracy(y_full_peng, qda.predict(X_full_peng))\n",
    "print(f\"Train (apparent) error is {1-train_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99b6611-398e-47b2-889d-2ee0def7d65f",
   "metadata": {},
   "source": [
    "### 2. B1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "41344e59-09f3-4b86-bd85-29055fd43bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (apparent) error is 0.0175\n"
     ]
    }
   ],
   "source": [
    "#Entrenamos un QDA y medimos su accuracy\n",
    "qda = QDA()\n",
    "qda.fit(X_full_peng, y_full_peng, a_priori=np.array([0.9, 0.05, 0.05]))\n",
    "train_acc = accuracy(y_full_peng, qda.predict(X_full_peng))\n",
    "print(f\"Train (apparent) error is {1-train_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e51d3f5-9c6a-498b-a014-f4f1feab354f",
   "metadata": {},
   "source": [
    "### 2. B2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "656c07ee-6d1e-4bad-960e-b73e71c71732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (apparent) error is 0.0351\n"
     ]
    }
   ],
   "source": [
    "#Entrenamos un QDA y medimos su accuracy\n",
    "qda = QDA()\n",
    "qda.fit(X_full_peng, y_full_peng, a_priori=np.array([0.05, 0.9, 0.05]))\n",
    "train_acc = accuracy(y_full_peng, qda.predict(X_full_peng))\n",
    "print(f\"Train (apparent) error is {1-train_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bd0b5b-f740-4c8e-ac59-b1326ade3693",
   "metadata": {},
   "source": [
    "### 2. B3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d0fa1f00-63ce-495a-93c5-88fb064c1fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (apparent) error is 0.0088\n"
     ]
    }
   ],
   "source": [
    "#Entrenamos un QDA y medimos su accuracy\n",
    "qda = QDA()\n",
    "qda.fit(X_full_peng, y_full_peng, a_priori=np.array([0.05, 0.05, 0.9]))\n",
    "train_acc = accuracy(y_full_peng, qda.predict(X_full_peng))\n",
    "print(f\"Train (apparent) error is {1-train_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055d2472-3d23-4b3e-85bc-00da24d21730",
   "metadata": {},
   "source": [
    "(RTA)  \n",
    "Los modelos que convergen al error aparente más bajo son el A (suponiendo distribución a priori uniforme) y B3 (suponiendo una distribución del 90% de probabilidad para la clase \"Gentoo\", la cual tiene una frecuencia relativa del 35,96% en el dataset).   \n",
    "Por otro lado, la distribución de probabilidad del caso B1 (90% para la clase Adeline, la cual tiene un 44,15% de frecuencia relativa en el dataset) dió un error mayor a los dos casos mencionados previamente. Esto se puede explicar porque la distribución a priori condiciona la predicción a la probabilidad que aparezca esa clase y, por las características del dataset, el modelo falla en las otras dos clases ya que discrimina peor entre sus atributos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e279facb-b7b7-4fa6-84f1-f9a2ad9f436a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f68c1c-4162-4aa4-8e91-9ec7f1e1d20d",
   "metadata": {},
   "source": [
    "3. Implementar el modelo LDA, entrenarlo y testearlo contra los mismos sets que QDA (no múltiples prioris) ¿Se observan diferencias? ¿Podría decirse que alguno de los dos es notoriamente mejor que el otro?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9e59aa-9943-43ab-b197-ef8419c64e79",
   "metadata": {},
   "source": [
    "_Por la consigna, se entinede que no debemos realizar un split del dataset, para mantener los mismos set que se usaron en QDA. A continuación se desarrolla la implementación de LDA para los set de Iris y Penguins con una distribución a priori de tipo Uniforme._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "dbe04cc8-4f80-4ba2-8884-120c39b3edf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se hereda la clase QDA modificando la predicción para una matriz de covarianza constante a lo largo de las clases\n",
    "\n",
    "class LDA(QDA):\n",
    "    \n",
    "    def _fit_params(self, X, y):\n",
    "    # estimate ONE covariance matrix\n",
    "    self.inv_covs = inv(np.cov(X, bias=True))\n",
    "    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True)\n",
    "                  for idx in range(len(self.log_a_priori))]\n",
    "    \n",
    "    def _predict_log_conditional(self, x, class_idx):\n",
    "    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n",
    "    # this should depend on the model used\n",
    "    inv_cov = self.inv_covs[class_idx]\n",
    "    #unbiased_x =  x - self.means[class_idx]\n",
    "    unbiased_x =  x - 0.5 * self.means[class_idx]\n",
    "      \n",
    "    #return 0.5*np.log(det(inv_cov)) -0.5 * unbiased_x.T @ inv_cov @ unbiased_x\n",
    "    return self.means[class_idx].T @ inv_cov @ unbiased_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93745b05-269b-4aee-9fee-304f6309a502",
   "metadata": {},
   "source": [
    "### LDA Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5f8f7700-014d-4f2c-9fbd-86e65a58fc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'setosa': 0, 'versicolor': 1, 'virginica': 2}\n",
      "Train (apparent) error for iris with Uniform prior is 0.6667\n"
     ]
    }
   ],
   "source": [
    "#Entrenamos un LDA y medimos su accuracy\n",
    "lda = LDA()\n",
    "names_class = lda.fit(X_full_iris, y_full_iris, a_priori=np.array([1/3, 1/3, 1/3]))\n",
    "print(names_class)\n",
    "train_acc = accuracy(y_full_iris, lda.predict(X_full_iris))\n",
    "print(f\"Train (apparent) error for iris with Uniform prior is {1-train_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93aa965-1a61-48b2-b0c7-ff3f3d756497",
   "metadata": {},
   "source": [
    "### LDA Penguins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9e2782d4-fbae-4a05-a47a-9d2e55c6aa47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Adelie': 0, 'Chinstrap': 1, 'Gentoo': 2}\n",
      "Train (apparent) error for iris with Uniform prior is 0.5585\n"
     ]
    }
   ],
   "source": [
    "#Entrenamos un LDA y medimos su accuracy\n",
    "lda = LDA()\n",
    "names_class = lda.fit(X_full_peng, y_full_peng, a_priori=np.array([1/3, 1/3, 1/3]))\n",
    "print(names_class)\n",
    "train_acc = accuracy(y_full_peng, lda.predict(X_full_peng))\n",
    "print(f\"Train (apparent) error for iris with Uniform prior is {1-train_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc321da6-9be0-4c25-b9b3-6f09620ddfb8",
   "metadata": {},
   "source": [
    "(RTA)  \n",
    "Los resultados de LDA sobre los mismo sets que se uso en QDA, para una distribución a priori uniforme, fueron ampliamente peores.  \n",
    "El error aparente sobre el set de entrenamiento creció en más de un orden de magnitud:  \n",
    " - 33 veces mayor para el dataset de Iris (0.6667/0.02)\n",
    " - 63 veces mayor para el dataset de Penguins (0.5585/0.0088)\n",
    "\n",
    "El hecho de asumir que las matrices de covarianza son iguales para todas las clases puede explicar el mal ajuste de los modelos LDA.  \n",
    "Por otro lado, el modelo de Análisis Discriminante Cuadrático (QDA) no lleva a cabo esta suposición y permite que cada clase tenga su propia matriz de covarianza. Por lo tanto puede capturar mejor la complejidad de la estructura de los datos cuando las clases tienen diferentes varianzas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce72bef-7833-4d34-8511-c381569b87a9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d4f12b-a64e-4692-b0fe-b5e2ab783184",
   "metadata": {},
   "source": [
    "4 Utilizar otros 2 (dos) valores de random seed para obtener distintos splits de train y test, y repetir la comparación del punto anterior ¿Las conclusiones previas se mantienen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ce7ccf32-d251-43c3-9e66-96b320e8731e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hiperparámetros\n",
    "rng_seeds = [125,99]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19be9cc-4675-4d36-bc8f-2846a8c689f2",
   "metadata": {},
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "1d3c865b-2dc2-40e3-ab80-8028a99ddd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing data, train - test validation\n",
    "# 70-30 split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_transpose(X, y, test_sz, random_state):\n",
    "    # split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_state)\n",
    "\n",
    "    # transpose so observations are column vectors\n",
    "    return X_train.T, y_train.T, X_test.T, y_test.T\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "  return (y_true == y_pred).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9d16d5-127e-4cbf-98fc-e4e043bfa14b",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "80786c45-d359-437b-bf22-3502ef83b903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:  125\n",
      "Iiris\n",
      "Train (apparent) error is 0.6762 while test error is 0.6444\n",
      "-------\n",
      "Penguins\n",
      "Train (apparent) error is 0.6318 while test error is 0.6602\n",
      "-------\n",
      "Seed:  99\n",
      "Iiris\n",
      "Train (apparent) error is 0.6381 while test error is 0.7333\n",
      "-------\n",
      "Penguins\n",
      "Train (apparent) error is 0.6192 while test error is 0.6893\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "for i,rng_seed in enumerate(rng_seeds):\n",
    "    print(\"Seed: \",rng_seed)\n",
    "    print(\"Iiris\")\n",
    "    train_x, train_y, test_x, test_y = split_transpose(X_full_iris.T, y_full_iris.T, 0.3, rng_seed)\n",
    "    \n",
    "    lda = LDA()\n",
    "    lda.fit(train_x, train_y, a_priori=np.array([1/3, 1/3, 1/3]))\n",
    "    \n",
    "    train_acc = accuracy(train_y, lda.predict(train_x))\n",
    "    test_acc = accuracy(test_y, lda.predict(test_x))\n",
    "    print(f\"Train (apparent) error is {1-train_acc:.4f} while test error is {1-test_acc:.4f}\")\n",
    "    \n",
    "    print(\"-------\")\n",
    "\n",
    "    print(\"Penguins\")\n",
    "    train_x, train_y, test_x, test_y = split_transpose(X_full_peng.T, y_full_peng.T, 0.3, rng_seed)\n",
    "    \n",
    "    lda = LDA()\n",
    "    lda.fit(train_x, train_y, a_priori=np.array([1/3, 1/3, 1/3]))\n",
    "    \n",
    "    train_acc = accuracy(train_y, lda.predict(train_x))\n",
    "    test_acc = accuracy(test_y, lda.predict(test_x))\n",
    "    print(f\"Train (apparent) error is {1-train_acc:.4f} while test error is {1-test_acc:.4f}\")\n",
    "    print(\"-------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee929ed2-e087-4646-958e-aec519df8ae9",
   "metadata": {},
   "source": [
    "### QDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "51ac5c8f-4cff-456f-87fe-5da93b0996b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:  125\n",
      "Iiris\n",
      "Train (apparent) error is 0.0286 while test error is 0.0000\n",
      "-------\n",
      "Penguins\n",
      "Train (apparent) error is 0.6318 while test error is 0.6602\n",
      "-------\n",
      "Seed:  99\n",
      "Iiris\n",
      "Train (apparent) error is 0.0190 while test error is 0.0222\n",
      "-------\n",
      "Penguins\n",
      "Train (apparent) error is 0.6192 while test error is 0.6893\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "for i,rng_seed in enumerate(rng_seeds):\n",
    "    print(\"Seed: \",rng_seed)\n",
    "    print(\"Iiris\")\n",
    "    train_x, train_y, test_x, test_y = split_transpose(X_full_iris.T, y_full_iris.T, 0.3, rng_seed)\n",
    "    \n",
    "    lda = QDA()\n",
    "    lda.fit(train_x, train_y, a_priori=np.array([1/3, 1/3, 1/3]))\n",
    "    \n",
    "    train_acc = accuracy(train_y, lda.predict(train_x))\n",
    "    test_acc = accuracy(test_y, lda.predict(test_x))\n",
    "    print(f\"Train (apparent) error is {1-train_acc:.4f} while test error is {1-test_acc:.4f}\")\n",
    "    \n",
    "    print(\"-------\")\n",
    "\n",
    "    print(\"Penguins\")\n",
    "    train_x, train_y, test_x, test_y = split_transpose(X_full_peng.T, y_full_peng.T, 0.3, rng_seed)\n",
    "    \n",
    "    lda = LDA()\n",
    "    lda.fit(train_x, train_y, a_priori=np.array([1/3, 1/3, 1/3]))\n",
    "    \n",
    "    train_acc = accuracy(train_y, lda.predict(train_x))\n",
    "    test_acc = accuracy(test_y, lda.predict(test_x))\n",
    "    print(f\"Train (apparent) error is {1-train_acc:.4f} while test error is {1-test_acc:.4f}\")\n",
    "    print(\"-------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c92560-6e80-4e1c-a868-e665f8f82187",
   "metadata": {},
   "source": [
    "### Saving data in a DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f3b08cdf-966a-4776-a7e8-8b80e53eda8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "03f640ab-2756-4205-ad1f-978704a2afd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Modelo</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Seed</th>\n",
       "      <th>Error (train)</th>\n",
       "      <th>Error (test)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>QDA</td>\n",
       "      <td>Iris</td>\n",
       "      <td>125</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>QDA</td>\n",
       "      <td>Penguins</td>\n",
       "      <td>125</td>\n",
       "      <td>0.008368</td>\n",
       "      <td>0.009709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LDA</td>\n",
       "      <td>Iris</td>\n",
       "      <td>125</td>\n",
       "      <td>0.676190</td>\n",
       "      <td>0.644444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LDA</td>\n",
       "      <td>Penguins</td>\n",
       "      <td>125</td>\n",
       "      <td>0.631799</td>\n",
       "      <td>0.660194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>QDA</td>\n",
       "      <td>Iris</td>\n",
       "      <td>99</td>\n",
       "      <td>0.019048</td>\n",
       "      <td>0.022222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>QDA</td>\n",
       "      <td>Penguins</td>\n",
       "      <td>99</td>\n",
       "      <td>0.008368</td>\n",
       "      <td>0.009709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LDA</td>\n",
       "      <td>Iris</td>\n",
       "      <td>99</td>\n",
       "      <td>0.638095</td>\n",
       "      <td>0.733333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LDA</td>\n",
       "      <td>Penguins</td>\n",
       "      <td>99</td>\n",
       "      <td>0.619247</td>\n",
       "      <td>0.689320</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Modelo   Dataset  Seed  Error (train)  Error (test)\n",
       "0    QDA      Iris   125       0.028571      0.000000\n",
       "1    QDA  Penguins   125       0.008368      0.009709\n",
       "2    LDA      Iris   125       0.676190      0.644444\n",
       "3    LDA  Penguins   125       0.631799      0.660194\n",
       "4    QDA      Iris    99       0.019048      0.022222\n",
       "5    QDA  Penguins    99       0.008368      0.009709\n",
       "6    LDA      Iris    99       0.638095      0.733333\n",
       "7    LDA  Penguins    99       0.619247      0.689320"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "modelos = []\n",
    "datasets = []\n",
    "seeds = []\n",
    "errores_train = []\n",
    "errores_test = []\n",
    "\n",
    "for rng_seed in rng_seeds:\n",
    "    train_x, train_y, test_x, test_y = split_transpose(X_full_iris.T, y_full_iris.T, 0.3, rng_seed)\n",
    "    lda = QDA()\n",
    "    lda.fit(train_x, train_y, a_priori=np.array([1/3, 1/3, 1/3]))\n",
    "    train_acc = accuracy(train_y, lda.predict(train_x))\n",
    "    test_acc = accuracy(test_y, lda.predict(test_x))\n",
    "    modelos.append('QDA')\n",
    "    datasets.append('Iris')\n",
    "    seeds.append(rng_seed)\n",
    "    errores_train.append(1-train_acc)\n",
    "    errores_test.append(1-test_acc)\n",
    "    #------------\n",
    "    train_x, train_y, test_x, test_y = split_transpose(X_full_peng.T, y_full_peng.T, 0.3, rng_seed)\n",
    "    lda = QDA()\n",
    "    lda.fit(train_x, train_y, a_priori=np.array([1/3, 1/3, 1/3]))\n",
    "    train_acc = accuracy(train_y, lda.predict(train_x))\n",
    "    test_acc = accuracy(test_y, lda.predict(test_x))\n",
    "    modelos.append('QDA')\n",
    "    datasets.append('Penguins')\n",
    "    seeds.append(rng_seed)\n",
    "    errores_train.append(1-train_acc)\n",
    "    errores_test.append(1-test_acc)\n",
    "    #------------\n",
    "    #------------\n",
    "    train_x, train_y, test_x, test_y = split_transpose(X_full_iris.T, y_full_iris.T, 0.3, rng_seed)\n",
    "    lda = LDA()\n",
    "    lda.fit(train_x, train_y, a_priori=np.array([1/3, 1/3, 1/3]))\n",
    "    train_acc = accuracy(train_y, lda.predict(train_x))\n",
    "    test_acc = accuracy(test_y, lda.predict(test_x))\n",
    "    modelos.append('LDA')\n",
    "    datasets.append('Iris')\n",
    "    seeds.append(rng_seed)\n",
    "    errores_train.append(1-train_acc)\n",
    "    errores_test.append(1-test_acc)\n",
    "    #------------\n",
    "    train_x, train_y, test_x, test_y = split_transpose(X_full_peng.T, y_full_peng.T, 0.3, rng_seed)\n",
    "    lda = LDA()\n",
    "    lda.fit(train_x, train_y, a_priori=np.array([1/3, 1/3, 1/3]))\n",
    "    train_acc = accuracy(train_y, lda.predict(train_x))\n",
    "    test_acc = accuracy(test_y, lda.predict(test_x))\n",
    "    modelos.append('LDA')\n",
    "    datasets.append('Penguins')\n",
    "    seeds.append(rng_seed)\n",
    "    errores_train.append(1-train_acc)\n",
    "    errores_test.append(1-test_acc)\n",
    "\n",
    "# Crear el DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Modelo': modelos,\n",
    "    'Dataset': datasets,\n",
    "    'Seed': seeds,\n",
    "    'Error (train)': errores_train,\n",
    "    'Error (test)': errores_test\n",
    "})\n",
    "\n",
    "# Imprimir la tabla\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec4f710-076d-4d7e-bd84-992d9da04b1c",
   "metadata": {},
   "source": [
    "(RTA)  \n",
    "Podemos ver que el modelo QDA para Iris tiene un error en testing es de 0 para el seed 125. Esto se debe a que estamos usando una distribución a priori uniforme sobre un dataset uniforme con un estimador de discriminante cuadradático. La estimación por máxima verosimilitud logra ajustar los parámetros y predice correctamente el set de testing para ambos seeds.  \n",
    "En cambio el modelo LDA no logra predecir correctamente en el set de testing obteniendo error superiores al 50%. Esto se debe a que la naturaleza lineal de su discriminante no le permite clasificar correctamente las clases. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcafed4f-232f-434b-a273-e3a224633187",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923e8a82-80ec-483e-b04c-44d2d0773bf1",
   "metadata": {},
   "source": [
    "5. Estimar y comparar los tiempos de predicción de las clases `QDA` y `TensorizedQDA`. De haber diferencias ¿Cuáles pueden ser las causas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "88ec0329-0314-4855-966f-dbe858232494",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorizedQDA(QDA):\n",
    "\n",
    "    def _fit_params(self, X, y):\n",
    "        # ask plain QDA to fit params\n",
    "        super()._fit_params(X,y)\n",
    "\n",
    "        # stack onto new dimension\n",
    "        self.tensor_inv_cov = np.stack(self.inv_covs)\n",
    "        self.tensor_means = np.stack(self.means)\n",
    "\n",
    "    def _predict_log_conditionals(self,x):\n",
    "        unbiased_x = x - self.tensor_means\n",
    "        inner_prod = unbiased_x.transpose(0,2,1) @ self.tensor_inv_cov @ unbiased_x\n",
    "\n",
    "        return 0.5*np.log(det(self.tensor_inv_cov)) - 0.5 * inner_prod.flatten()\n",
    "\n",
    "    def _predict_one(self, x):\n",
    "        # return the class that has maximum a posteriori probability\n",
    "        return np.argmax(self.log_a_priori + self._predict_log_conditionals(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "6ca23a24-3237-4e0b-b592-1a603d991096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.7 ms ± 401 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "model_qda = QDA()\n",
    "model_qda.fit(train_x, train_y)\n",
    "model_qda.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "4a5dcb52-87ad-418d-b51a-b9681e948541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.37 ms ± 166 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "model_Tqda = TensorizedQDA()\n",
    "model_Tqda.fit(train_x, train_y)\n",
    "model_Tqda.predict(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0a524f-60d3-4c8b-b91d-d21f27716833",
   "metadata": {},
   "source": [
    "_midiendo solo la predicción_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "13914a54-0f23-4052-b386-88743c348d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Adelie': 0, 'Chinstrap': 1, 'Gentoo': 2}"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_qda = QDA()\n",
    "model_qda.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "733010f5-0237-4663-bd76-65f261bb4092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.8 ms ± 750 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "model_qda.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "adafecdc-b34d-49ea-93ac-4b313095fe22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Adelie': 0, 'Chinstrap': 1, 'Gentoo': 2}"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_Tqda = TensorizedQDA()\n",
    "model_Tqda.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "ecf25160-aa68-4a2e-8dfe-c9b4536250cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.61 ms ± 115 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "model_Tqda.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "d096456c-3980-4757-9376-1f58d908f724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.5596529284164857, 2.364990689013035)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "11.8/4.61 , 12.7/5.37"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ae0ff4-df8b-4c7a-9049-f2775d215d04",
   "metadata": {},
   "source": [
    "(RTA)  \n",
    "Viendo los tiempos obtenidos en las 4 mediciones, se concluye que la diferencia reside en la predicción. Se puede ver que la clase que utiliza tensores para la predicción (cálculo de la probabilidad a posteriori) es más del doble de rápida que la clase QDA standard.  \n",
    "Esto se explica comparando los métodos \"_predict_log_conditionals\":  \n",
    " - En QDA tenemos productos matriciales, los cuales se ven mediante el operador \"@\".\n",
    " - En TensorizedQDA tenemos operaciones element-wise que solamente utilizan operadores \"+\" y \"*\". Para poder hacer esto se utiliza el tensor \"tensor_inv_cov\" que contiene las matrices de covarianza inversas de todas las clases. También se usa el operador @, especificamente en \"inner_prod\", pero como las variables que computa son tensores, numpy realiza las operaciones elemento a elemento.\n",
    "\n",
    "Si computaramos cantidad de iteraciones en ves del tiempo, la diferencia sería 3 a 1 para los sets utilizados. En este caso vemos una diferencia de 2,5 aproximadamente ya que la predicción para una clase demora levemente menos que para las tres en simultaneo. QDA realiza la probabilidad condicional para una sola observación y una sola clase a la vez mientras que TensorizedQDA realiza el cálculo para todas las clases simultáneamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f349de-ab9a-4214-81d3-b0ae03331b23",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df60e667-5bae-405f-96b9-75e9d040010a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed800d99-ca57-4d5e-a2ee-ebafcc07d148",
   "metadata": {},
   "source": [
    "# Optimización matemática"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90920a06-813a-4eae-92a5-fbcd3411c796",
   "metadata": {},
   "source": [
    "## QDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f128b8-1d97-4290-8436-cb850a8db3ae",
   "metadata": {},
   "source": [
    "Debido a la forma cuadrática de QDA, no se puede predecir para *n* observaciones en una sola pasada (utilizar $X \\in \\mathbb{R}^{p \\times n}$ en vez de $x \\in \\mathbb{R}^p$) sin pasar por una matriz de *n x n* en donde se computan todas las interacciones entre observaciones. Se puede acceder al resultado recuperando sólo la diagonal de dicha matriz, pero resulta ineficiente en tiempo y (especialmente) en memoria. Aún así, es *posible* que el modelo funcione más rápido.\n",
    "\n",
    "1. Implementar el modelo `FasterQDA` (se recomienda heredarlo de TensorizedQDA) de manera de eliminar el ciclo for en el método predict.\n",
    "2. Comparar los tiempos de predicción de `FasterQDA` con `TensorizedQDA` y `QDA`.\n",
    "3. Mostrar (puede ser con un print) dónde aparece la mencionada matriz de *n x n*, donde *n* es la cantidad de observaciones a predecir.\n",
    "4.Demostrar que\n",
    "$$\n",
    "diag(A \\cdot B) = \\sum_{cols} A \\odot B^T = np.sum(A \\odot B^T, axis=1)\n",
    "$$ es decir, que se puede \"esquivar\" la matriz de *n x n* usando matrices de *n x p*.\n",
    "5.Utilizar la propiedad antes demostrada para reimplementar la predicción del modelo `FasterQDA` de forma eficiente. ¿Hay cambios en los tiempos de predicción?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6e8542-49fc-4e1a-9335-78d957f498a5",
   "metadata": {},
   "source": [
    "## LDA\n",
    "\n",
    "1. \"Tensorizar\" el modelo LDA y comparar sus tiempos de predicción con el modelo antes implementado. *Notar que, en modo tensorizado, se puede directamente precomputar $\\mu^T \\cdot \\Sigma^{-1} \\in \\mathbb{R}^{k \\times 1 \\times p}$ y guardar eso en vez de $\\Sigma^{-1}$.*\n",
    "2. LDA no sufre del problema antes descrito de QDA debido a que no computa productos internos, por lo que no tiene un verdadero costo extra en memoria predecir \"en batch\". Implementar el modelo `FasterLDA` y comparar sus tiempos de predicción con las versiones anteriores de LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc77fcf-c864-45e4-9bb0-c609e27eb889",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
